{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e:\\\\Study\\\\COMPLETED PROJECTS\\\\End-to-End-Source-Code-Analysis-Gen-Ai\\\\research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "repo = Repo.clone_from(\"https://github.com/paravsyal02/End-to-End-Medical-Chatbot.git\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = GenericLoader.from_filesystem(repo_path, \n",
    "                                       glob = \"**/*\",\n",
    "                                        suffixes=[\".py\"],\n",
    "                                        parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)\\n\\n# Load environment variables\\nload_dotenv()\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone Index\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever()\\n\\nllm = ChatGoogleGenerativeAI(\\n    model=\"gemini-1.5-flash\",\\n    temperature=0.6,\\n    timeout=None,\\n    max_retries=2,\\n    api_key=GOOGLE_API_KEY,\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")\\ndef index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"POST\"])\\ndef chat():\\n    msg = request.form[\"msg\"]\\n    print(f\"Received message: {msg}\")  # Debugging the input\\n\\n    # Retrieve relevant documents from Pinecone vector store\\n    retrieved_docs = retriever.get_relevant_documents(msg)\\n    print(f\"Retrieved documents: {retrieved_docs}\")  # Debugging retrieved docs\\n\\n    # Generate response using the RAG (retrieval-augmented generation) chain\\n    response = rag_chain.invoke({\"input\": msg, \"context\": retrieved_docs})\\n    print(f\"Response: {response[\\'answer\\']}\")  # Debugging response\\n\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='import streamlit as st\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nimport os\\n\\n# Load environment variables\\nload_dotenv()\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\n\\n# Initialize embeddings\\nembeddings = download_hugging_face_embeddings()\\n\\n# Define Pinecone index name\\nindex_name = \"medicalbot\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\nretriever = docsearch.as_retriever()\\n\\n# Initialize LLM\\nllm = ChatGoogleGenerativeAI(\\n    model=\"gemini-1.5-flash\",\\n    temperature=0.6,\\n    timeout=None,\\n    max_retries=2,\\n    api_key=GOOGLE_API_KEY  # Ensure you\\'re using environment variables, not hardcoding API keys\\n)\\n\\n# Define the prompt\\nsystem_prompt = \"\"\"\\n    You are an assistant for question-answering tasks.\\n    Use the following retrieved context to answer the question.\\n    If you don\\'t know the answer, say that you don\\'t know.\\n    Keep the answer detailed yet concise.\\n    \\\\n\\\\n\\n    {context}\\n\"\"\"\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt),\\n    (\"human\", \"{input}\"),\\n])\\n\\n# Create the retrieval-augmented generation (RAG) chain\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n# Streamlit UI\\nst.set_page_config(page_title=\"Medical Chatbot\", layout=\"wide\")\\nst.title(\"ðŸ©º AI-Powered Medical Chatbot\")\\nst.write(\"Ask me any medical-related questions!\")\\n\\n# Chat input\\nuser_input = st.text_input(\"Type your question here:\", \"\")\\n\\nif st.button(\"Ask AI\") and user_input:\\n    retrieved_docs = retriever.invoke(user_input)\\n    response = rag_chain.invoke({\"input\": user_input, \"context\": retrieved_docs})\\n    \\n    st.subheader(\"Chatbot Response:\")\\n    st.write(response[\"answer\"])\\n    '),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name='GEN AI Project',\\n    version='0.0.0',\\n    author='Parav Syal',\\n    author_email='paravsyal02@gmail.com',\\n    packages=find_packages(),\\n    install_requires=[],\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\\n\\n\\nextracted_data = load_pdf_file(data=\\'Data/\\')\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)\\n\\n\\n#Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks, \\n    index_name=index_name, \\n    embedding=embeddings,\\n)\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)\\n\\n\\n    if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_huggingface import HuggingFaceEmbeddings  \\n\\n# Extract data from the pdf file\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    \\n    documents=loader.load()\\n\\n    return documents\\n\\n# Split the data into text chunks\\ndef text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks  = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# Download the embeddings from the huggingface\\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name=\\'sentence-transformers/all-MiniLM-L6-v2\\')\\n    return embeddings'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='\\nsystem_prompt = (\"\"\"\\n    You are an assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer the question. If you\\n    don\\'t know the answer, say that you don\\'t know. Give the answer in detail \\n    and give the answer very concise and to the point.\\n    \\\\n\\\\n\\n    {context}\"\"\"\\n)\\n\\n'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\__init__.py', 'language': <Language.PYTHON: 'python'>}, page_content='')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.PYTHON,\n",
    "                                                                  chunk_size = 500,\n",
    "                                                                  chunk_overlap = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='from flask import Flask, render_template, jsonify, request\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nfrom src.prompt import *\\nimport os\\n\\napp = Flask(__name__)'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Load environment variables\\nload_dotenv()\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\n\\nembeddings = download_hugging_face_embeddings()\\n\\nindex_name = \"medicalbot\"\\n\\n# Embed each chunk and upsert the embeddings into your Pinecone Index\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\n\\nretriever = docsearch.as_retriever()'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='llm = ChatGoogleGenerativeAI(\\n    model=\"gemini-1.5-flash\",\\n    temperature=0.6,\\n    timeout=None,\\n    max_retries=2,\\n    api_key=GOOGLE_API_KEY,\\n)\\n\\nprompt = ChatPromptTemplate.from_messages(\\n    [\\n        (\"system\", system_prompt),\\n        (\"human\", \"{input}\"),\\n    ]\\n)\\n\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n\\n@app.route(\"/\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def index():\\n    return render_template(\\'chat.html\\')\\n\\n\\n@app.route(\"/get\", methods=[\"POST\"])'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='def chat():\\n    msg = request.form[\"msg\"]\\n    print(f\"Received message: {msg}\")  # Debugging the input\\n\\n    # Retrieve relevant documents from Pinecone vector store\\n    retrieved_docs = retriever.get_relevant_documents(msg)\\n    print(f\"Retrieved documents: {retrieved_docs}\")  # Debugging retrieved docs'),\n",
       " Document(metadata={'source': 'test_repo\\\\app.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Generate response using the RAG (retrieval-augmented generation) chain\\n    response = rag_chain.invoke({\"input\": msg, \"context\": retrieved_docs})\\n    print(f\"Response: {response[\\'answer\\']}\")  # Debugging response\\n\\n    return str(response[\"answer\"])\\n\\n\\nif __name__ == \"__main__\":\\n    app.run(host=\"0.0.0.0\", port=8080, debug=True)'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='import streamlit as st\\nfrom src.helper import download_hugging_face_embeddings\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom langchain_google_genai import ChatGoogleGenerativeAI\\nfrom langchain.chains import create_retrieval_chain\\nfrom langchain.chains.combine_documents import create_stuff_documents_chain\\nfrom langchain.prompts import ChatPromptTemplate\\nfrom dotenv import load_dotenv\\nimport os'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Load environment variables\\nload_dotenv()\\nPINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\\nGOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\\n\\n# Initialize embeddings\\nembeddings = download_hugging_face_embeddings()\\n\\n# Define Pinecone index name\\nindex_name = \"medicalbot\"\\n\\ndocsearch = PineconeVectorStore.from_existing_index(\\n    index_name=index_name,\\n    embedding=embeddings\\n)\\nretriever = docsearch.as_retriever()'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Initialize LLM\\nllm = ChatGoogleGenerativeAI(\\n    model=\"gemini-1.5-flash\",\\n    temperature=0.6,\\n    timeout=None,\\n    max_retries=2,\\n    api_key=GOOGLE_API_KEY  # Ensure you\\'re using environment variables, not hardcoding API keys\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Define the prompt\\nsystem_prompt = \"\"\"\\n    You are an assistant for question-answering tasks.\\n    Use the following retrieved context to answer the question.\\n    If you don\\'t know the answer, say that you don\\'t know.\\n    Keep the answer detailed yet concise.\\n    \\\\n\\\\n\\n    {context}\\n\"\"\"\\n\\nprompt = ChatPromptTemplate.from_messages([\\n    (\"system\", system_prompt),\\n    (\"human\", \"{input}\"),\\n])'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='# Create the retrieval-augmented generation (RAG) chain\\nquestion_answer_chain = create_stuff_documents_chain(llm, prompt)\\nrag_chain = create_retrieval_chain(retriever, question_answer_chain)\\n\\n# Streamlit UI\\nst.set_page_config(page_title=\"Medical Chatbot\", layout=\"wide\")\\nst.title(\"ðŸ©º AI-Powered Medical Chatbot\")\\nst.write(\"Ask me any medical-related questions!\")\\n\\n# Chat input\\nuser_input = st.text_input(\"Type your question here:\", \"\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\appwithstreamlit.py', 'language': <Language.PYTHON: 'python'>}, page_content='if st.button(\"Ask AI\") and user_input:\\n    retrieved_docs = retriever.invoke(user_input)\\n    response = rag_chain.invoke({\"input\": user_input, \"context\": retrieved_docs})\\n    \\n    st.subheader(\"Chatbot Response:\")\\n    st.write(response[\"answer\"])'),\n",
       " Document(metadata={'source': 'test_repo\\\\setup.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"from setuptools import find_packages, setup\\n\\nsetup(\\n    name='GEN AI Project',\\n    version='0.0.0',\\n    author='Parav Syal',\\n    author_email='paravsyal02@gmail.com',\\n    packages=find_packages(),\\n    install_requires=[],\\n)\"),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='from src.helper import load_pdf_file, text_split, download_hugging_face_embeddings\\nfrom pinecone.grpc import PineconeGRPC as Pinecone\\nfrom pinecone import ServerlessSpec\\nfrom langchain_pinecone import PineconeVectorStore\\nfrom dotenv import load_dotenv\\nimport os\\n\\n\\nload_dotenv()\\n\\nPINECONE_API_KEY = os.environ.get(\"PINECONE_API_KEY\")\\nos.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='extracted_data = load_pdf_file(data=\\'Data/\\')\\ntext_chunks = text_split(extracted_data)\\nembeddings = download_hugging_face_embeddings()\\n\\n\\npc = Pinecone(api_key=PINECONE_API_KEY)\\n\\nindex_name = \"medicalbot\"\\n\\npc.create_index(\\n    name=index_name,\\n    dimension=384, # Replace with your model dimensions\\n    metric=\"cosine\", # Replace with your model metric\\n    spec=ServerlessSpec(\\n        cloud=\"aws\",\\n        region=\"us-east-1\"\\n    ) \\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\store_index.py', 'language': <Language.PYTHON: 'python'>}, page_content='#Embed each chunk and upsert the embeddings into your Pinecone index.\\ndocsearch = PineconeVectorStore.from_documents(\\n    documents=text_chunks, \\n    index_name=index_name, \\n    embedding=embeddings,\\n)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='import os\\nfrom pathlib import Path\\nimport logging\\n\\nlogging.basicConfig(level=logging.INFO, format=\\'[%(asctime)s]: %(message)s:\\')\\n\\nlist_of_files = [\\n    \"src/__init__.py\",\\n    \"src/helper.py\",\\n    \"src/prompt.py\",\\n    \".env\",\\n    \"setup.py\",\\n    \"app.py\",\\n    \"research/trials.ipynb\"\\n]\\n\\nfor filepath in list_of_files:\\n    filepath = Path(filepath)\\n    filedir, filename = os.path.split(filepath)'),\n",
       " Document(metadata={'source': 'test_repo\\\\template.py', 'language': <Language.PYTHON: 'python'>}, page_content='if filedir !=\"\":\\n        os.makedirs(filedir, exist_ok=True)\\n        logging.info(f\"Creating directory; {filedir} for the file: {filename}\")\\n\\n    if (not os.path.exists(filepath)) or (os.path.getsize(filepath) == 0):\\n        with open(filepath, \"w\") as f:\\n            pass\\n            logging.info(f\"Creating empty file: {filepath}\")\\n\\n    else:\\n        logging.info(f\"{filename} already exists\")'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content='from langchain_community.document_loaders import PyPDFLoader, DirectoryLoader\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain_huggingface import HuggingFaceEmbeddings  \\n\\n# Extract data from the pdf file\\ndef load_pdf_file(data):\\n    loader = DirectoryLoader(data,\\n                             glob=\"*.pdf\",\\n                             loader_cls=PyPDFLoader)\\n    \\n    documents=loader.load()\\n\\n    return documents\\n\\n# Split the data into text chunks'),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\helper.py', 'language': <Language.PYTHON: 'python'>}, page_content=\"def text_split(extracted_data):\\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\\n    text_chunks  = text_splitter.split_documents(extracted_data)\\n    return text_chunks\\n\\n# Download the embeddings from the huggingface\\ndef download_hugging_face_embeddings():\\n    embeddings=HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\\n    return embeddings\"),\n",
       " Document(metadata={'source': 'test_repo\\\\src\\\\prompt.py', 'language': <Language.PYTHON: 'python'>}, page_content='system_prompt = (\"\"\"\\n    You are an assistant for question-answering tasks.\\n    Use the following pieces of retrieved context to answer the question. If you\\n    don\\'t know the answer, say that you don\\'t know. Give the answer in detail \\n    and give the answer very concise and to the point.\\n    \\\\n\\\\n\\n    {context}\"\"\"\\n)')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3040\\3711397106.py:1: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectordb.persist()\n"
     ]
    }
   ],
   "source": [
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory( memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwags ={\"k\":8}, memory=memory ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is download_hugging_face_embeddings function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_pdf_file` function loads all PDF files from a specified directory. It uses a `DirectoryLoader` with the `PyPDFLoader` to handle PDF parsing.  It returns a list of `Document` objects, each representing a loaded PDF file.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": question, \"chat_history\": []})  # Ensure chat_history is included\n",
    "print(result['answer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is load_pdf_file function\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The `load_pdf_file` function loads all PDF files from a specified directory. It uses the `DirectoryLoader` from `langchain_community.document_loaders`, specifying `PyPDFLoader` to handle PDF files. The function returns a list of `Document` objects, each representing a loaded PDF.\n"
     ]
    }
   ],
   "source": [
    "result = qa({\"question\": question, \"chat_history\": []})  # Ensure chat_history is included\n",
    "print(result['answer'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "source",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
